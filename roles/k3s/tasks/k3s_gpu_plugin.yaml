- name: Install NVIDIA device plugin Helm chart
  shell: |
    #helm repo add nvidia-device-plugin https://nvidia.github.io/k8s-device-plugin
    #helm repo update
    #helm install nvidia-device-plugin nvidia-device-plugin/nvidia-device-plugin --set runtimeClassName=nvidia --namespace kube-system
    helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
    helm repo update
    helm upgrade --install --wait nvidiagpu -n gpu-operator --create-namespace            \
         --set toolkit.env[0].name=CONTAINERD_CONFIG                                      \
         --set toolkit.env[0].value=/var/lib/rancher/k3s/agent/etc/containerd/config.toml \
         --set toolkit.env[1].name=CONTAINERD_SOCKET                                      \
         --set toolkit.env[1].value=/run/k3s/containerd/containerd.sock                   \
         --set toolkit.env[2].name=CONTAINERD_RUNTIME_CLASS                               \
         --set toolkit.env[2].value=nvidia                                                \
         --set toolkit.env[3].name=CONTAINERD_SET_AS_DEFAULT                              \
         --set-string toolkit.env[3].value=true                                           \
         nvidia/gpu-operator

- name: Verify RuntimeClass and Pod
  k8s:
    state: present
    definition:
      apiVersion: node.k8s.io/v1
      kind: RuntimeClass
      metadata:
        name: nvidia
      handler: nvidia
  register: runtime_class_result

- name: Verify Pod with NVIDIA GPU
  k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Pod
      metadata:
        name: nbody-gpu-benchmark
        namespace: default
      spec:
        restartPolicy: OnFailure
        runtimeClassName: nvidia
        tolerations:
          - key: nvidia.com/gpu  
            operator: Exists
            effect: NoSchedule
        containers:
          - name: cuda-container
            image: nvcr.io/nvidia/k8s/cuda-sample:nbody-cuda11.2.1
            args: ["nbody", "-gpu", "-benchmark"]
            resources:
              limits:
                nvidia.com/gpu: 1
            env:
              - name: NVIDIA_VISIBLE_DEVICES
                value: all
              - name: NVIDIA_DRIVER_CAPABILITIES
                value: all
  register: pod_result_nvidia_smi_usage

- name: Verify Pod CUDA operations with NVIDIA GPU
  k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Pod
      metadata:
        name: vec-add-pod
      spec:
        restartPolicy: OnFailure
        runtimeClassName: nvidia
        containers:
          - name: cuda-vector-add
            # https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile
            image: "k8s.gcr.io/cuda-vector-add:v0.1"
            resources:
              limits:
                nvidia.com/gpu: 1
  register: pod_result_cuda_operations

- name: Output results
  debug:
    msg: "{{ item }}"
  loop:
    - "{{ runtime_class_result }}"
    - "{{ pod_result_nvidia_smi_usage }}"
    - "{{ pod_result_cuda_operations }}"
